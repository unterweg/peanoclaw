#ifndef _PEANOCLAW_RECORDS_STATE_H
#define _PEANOCLAW_RECORDS_STATE_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>
#include "peano/utils/Globals.h"

namespace peanoclaw {
   namespace records {
      class State;
      class StatePacked;
   }
}

#if defined(Parallel)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   12/06/2014 07:41
    */
   class peanoclaw::records::State { 
      
      public:
         
         typedef peanoclaw::records::StatePacked Packed;
         
         struct PersistentRecords {
            bool _isInitializing;
            bool _isRefinementCriterionEnabled;
            int _unknownsPerSubcell;
            int _numberOfParametersWithoutGhostlayerPerSubcell;
            int _numberOfParametersWithGhostlayerPerSubcell;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _initialMaximalSubgridSize __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _initialMaximalSubgridSize;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,int> _defaultSubdivisionFactor __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,int> _defaultSubdivisionFactor;
            #endif
            int _defaultGhostWidthLayer;
            double _initialTimestepSize;
            bool _useDimensionalSplittingExtrapolation;
            double _globalTimestepEndTime;
            bool _allPatchesEvolvedToGlobalTimestep;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _domainOffset __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _domainOffset;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _domainSize __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _domainSize;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<32,int> _plotName __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<32,int> _plotName;
            #endif
            int _plotNumber;
            double _startMaximumGlobalTimeInterval;
            double _endMaximumGlobalTimeInterval;
            double _startMinimumGlobalTimeInterval;
            double _endMinimumGlobalTimeInterval;
            double _minimalTimestep;
            double _totalNumberOfCellUpdates;
            bool _reduceReductions;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _minMeshWidth __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _minMeshWidth;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _maxMeshWidth __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _maxMeshWidth;
            #endif
            double _numberOfInnerVertices;
            double _numberOfBoundaryVertices;
            double _numberOfOuterVertices;
            double _numberOfInnerCells;
            double _numberOfOuterCells;
            double _numberOfInnerLeafVertices;
            double _numberOfBoundaryLeafVertices;
            double _numberOfOuterLeafVertices;
            double _numberOfInnerLeafCells;
            double _numberOfOuterLeafCells;
            int _maxLevel;
            bool _hasRefined;
            bool _hasTriggeredRefinementForNextIteration;
            bool _hasErased;
            bool _hasTriggeredEraseForNextIteration;
            bool _hasChangedVertexOrCellState;
            bool _isTraversalInverted;
            bool _reduceStateAndCell;
            bool _couldNotEraseDueToDecompositionFlag;
            bool _subWorkerIsInvolvedInJoinOrFork;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const bool& reduceReductions, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted, const bool& reduceStateAndCell, const bool& couldNotEraseDueToDecompositionFlag, const bool& subWorkerIsInvolvedInJoinOrFork);
            
            
            inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInitializing;
            }
            
            
            
            inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInitializing = isInitializing;
            }
            
            
            
            inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isRefinementCriterionEnabled;
            }
            
            
            
            inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isRefinementCriterionEnabled = isRefinementCriterionEnabled;
            }
            
            
            
            inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _unknownsPerSubcell;
            }
            
            
            
            inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _unknownsPerSubcell = unknownsPerSubcell;
            }
            
            
            
            inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfParametersWithoutGhostlayerPerSubcell;
            }
            
            
            
            inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfParametersWithoutGhostlayerPerSubcell = numberOfParametersWithoutGhostlayerPerSubcell;
            }
            
            
            
            inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfParametersWithGhostlayerPerSubcell;
            }
            
            
            
            inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfParametersWithGhostlayerPerSubcell = numberOfParametersWithGhostlayerPerSubcell;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _initialMaximalSubgridSize;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _initialMaximalSubgridSize = (initialMaximalSubgridSize);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _defaultSubdivisionFactor;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _defaultSubdivisionFactor = (defaultSubdivisionFactor);
            }
            
            
            
            inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _defaultGhostWidthLayer;
            }
            
            
            
            inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _defaultGhostWidthLayer = defaultGhostWidthLayer;
            }
            
            
            
            inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _initialTimestepSize;
            }
            
            
            
            inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _initialTimestepSize = initialTimestepSize;
            }
            
            
            
            inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _useDimensionalSplittingExtrapolation;
            }
            
            
            
            inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _useDimensionalSplittingExtrapolation = useDimensionalSplittingExtrapolation;
            }
            
            
            
            inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _globalTimestepEndTime;
            }
            
            
            
            inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _globalTimestepEndTime = globalTimestepEndTime;
            }
            
            
            
            inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _allPatchesEvolvedToGlobalTimestep;
            }
            
            
            
            inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _allPatchesEvolvedToGlobalTimestep = allPatchesEvolvedToGlobalTimestep;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _domainOffset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _domainOffset = (domainOffset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _domainSize;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _domainSize = (domainSize);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _plotName;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _plotName = (plotName);
            }
            
            
            
            inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _plotNumber;
            }
            
            
            
            inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _plotNumber = plotNumber;
            }
            
            
            
            inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _startMaximumGlobalTimeInterval;
            }
            
            
            
            inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
            }
            
            
            
            inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _endMaximumGlobalTimeInterval;
            }
            
            
            
            inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
            }
            
            
            
            inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _startMinimumGlobalTimeInterval;
            }
            
            
            
            inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
            }
            
            
            
            inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _endMinimumGlobalTimeInterval;
            }
            
            
            
            inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
            }
            
            
            
            inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _minimalTimestep;
            }
            
            
            
            inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _minimalTimestep = minimalTimestep;
            }
            
            
            
            inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _totalNumberOfCellUpdates;
            }
            
            
            
            inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _totalNumberOfCellUpdates = totalNumberOfCellUpdates;
            }
            
            
            
            inline bool getReduceReductions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _reduceReductions;
            }
            
            
            
            inline void setReduceReductions(const bool& reduceReductions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _reduceReductions = reduceReductions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _minMeshWidth;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _minMeshWidth = (minMeshWidth);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _maxMeshWidth;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _maxMeshWidth = (maxMeshWidth);
            }
            
            
            
            inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfInnerVertices;
            }
            
            
            
            inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfInnerVertices = numberOfInnerVertices;
            }
            
            
            
            inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfBoundaryVertices;
            }
            
            
            
            inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfBoundaryVertices = numberOfBoundaryVertices;
            }
            
            
            
            inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfOuterVertices;
            }
            
            
            
            inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfOuterVertices = numberOfOuterVertices;
            }
            
            
            
            inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfInnerCells;
            }
            
            
            
            inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfInnerCells = numberOfInnerCells;
            }
            
            
            
            inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfOuterCells;
            }
            
            
            
            inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfOuterCells = numberOfOuterCells;
            }
            
            
            
            inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfInnerLeafVertices;
            }
            
            
            
            inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfInnerLeafVertices = numberOfInnerLeafVertices;
            }
            
            
            
            inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfBoundaryLeafVertices;
            }
            
            
            
            inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
            }
            
            
            
            inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfOuterLeafVertices;
            }
            
            
            
            inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfOuterLeafVertices = numberOfOuterLeafVertices;
            }
            
            
            
            inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfInnerLeafCells;
            }
            
            
            
            inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfInnerLeafCells = numberOfInnerLeafCells;
            }
            
            
            
            inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfOuterLeafCells;
            }
            
            
            
            inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfOuterLeafCells = numberOfOuterLeafCells;
            }
            
            
            
            inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _maxLevel;
            }
            
            
            
            inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _maxLevel = maxLevel;
            }
            
            
            
            inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasRefined;
            }
            
            
            
            inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasRefined = hasRefined;
            }
            
            
            
            inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasTriggeredRefinementForNextIteration;
            }
            
            
            
            inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasTriggeredRefinementForNextIteration = hasTriggeredRefinementForNextIteration;
            }
            
            
            
            inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasErased;
            }
            
            
            
            inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasErased = hasErased;
            }
            
            
            
            inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasTriggeredEraseForNextIteration;
            }
            
            
            
            inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasTriggeredEraseForNextIteration = hasTriggeredEraseForNextIteration;
            }
            
            
            
            inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasChangedVertexOrCellState;
            }
            
            
            
            inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasChangedVertexOrCellState = hasChangedVertexOrCellState;
            }
            
            
            
            inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isTraversalInverted;
            }
            
            
            
            inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isTraversalInverted = isTraversalInverted;
            }
            
            
            
            inline bool getReduceStateAndCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _reduceStateAndCell;
            }
            
            
            
            inline void setReduceStateAndCell(const bool& reduceStateAndCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _reduceStateAndCell = reduceStateAndCell;
            }
            
            
            
            inline bool getCouldNotEraseDueToDecompositionFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _couldNotEraseDueToDecompositionFlag;
            }
            
            
            
            inline void setCouldNotEraseDueToDecompositionFlag(const bool& couldNotEraseDueToDecompositionFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _couldNotEraseDueToDecompositionFlag = couldNotEraseDueToDecompositionFlag;
            }
            
            
            
            inline bool getSubWorkerIsInvolvedInJoinOrFork() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _subWorkerIsInvolvedInJoinOrFork;
            }
            
            
            
            inline void setSubWorkerIsInvolvedInJoinOrFork(const bool& subWorkerIsInvolvedInJoinOrFork) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _subWorkerIsInvolvedInJoinOrFork = subWorkerIsInvolvedInJoinOrFork;
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         State();
         
         /**
          * Generated
          */
         State(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         State(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const bool& reduceReductions, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted, const bool& reduceStateAndCell, const bool& couldNotEraseDueToDecompositionFlag, const bool& subWorkerIsInvolvedInJoinOrFork);
         
         /**
          * Generated
          */
         virtual ~State();
         
         
         inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isInitializing;
         }
         
         
         
         inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isInitializing = isInitializing;
         }
         
         
         
         inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isRefinementCriterionEnabled;
         }
         
         
         
         inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isRefinementCriterionEnabled = isRefinementCriterionEnabled;
         }
         
         
         
         inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._unknownsPerSubcell;
         }
         
         
         
         inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._unknownsPerSubcell = unknownsPerSubcell;
         }
         
         
         
         inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfParametersWithoutGhostlayerPerSubcell;
         }
         
         
         
         inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfParametersWithoutGhostlayerPerSubcell = numberOfParametersWithoutGhostlayerPerSubcell;
         }
         
         
         
         inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfParametersWithGhostlayerPerSubcell;
         }
         
         
         
         inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfParametersWithGhostlayerPerSubcell = numberOfParametersWithGhostlayerPerSubcell;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._initialMaximalSubgridSize;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._initialMaximalSubgridSize = (initialMaximalSubgridSize);
         }
         
         
         
         inline double getInitialMaximalSubgridSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._initialMaximalSubgridSize[elementIndex];
            
         }
         
         
         
         inline void setInitialMaximalSubgridSize(int elementIndex, const double& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._initialMaximalSubgridSize[elementIndex]= initialMaximalSubgridSize;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._defaultSubdivisionFactor;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._defaultSubdivisionFactor = (defaultSubdivisionFactor);
         }
         
         
         
         inline int getDefaultSubdivisionFactor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._defaultSubdivisionFactor[elementIndex];
            
         }
         
         
         
         inline void setDefaultSubdivisionFactor(int elementIndex, const int& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._defaultSubdivisionFactor[elementIndex]= defaultSubdivisionFactor;
            
         }
         
         
         
         inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._defaultGhostWidthLayer;
         }
         
         
         
         inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._defaultGhostWidthLayer = defaultGhostWidthLayer;
         }
         
         
         
         inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._initialTimestepSize;
         }
         
         
         
         inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._initialTimestepSize = initialTimestepSize;
         }
         
         
         
         inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._useDimensionalSplittingExtrapolation;
         }
         
         
         
         inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._useDimensionalSplittingExtrapolation = useDimensionalSplittingExtrapolation;
         }
         
         
         
         inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._globalTimestepEndTime;
         }
         
         
         
         inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._globalTimestepEndTime = globalTimestepEndTime;
         }
         
         
         
         inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._allPatchesEvolvedToGlobalTimestep;
         }
         
         
         
         inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._allPatchesEvolvedToGlobalTimestep = allPatchesEvolvedToGlobalTimestep;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._domainOffset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._domainOffset = (domainOffset);
         }
         
         
         
         inline double getDomainOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._domainOffset[elementIndex];
            
         }
         
         
         
         inline void setDomainOffset(int elementIndex, const double& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._domainOffset[elementIndex]= domainOffset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._domainSize;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._domainSize = (domainSize);
         }
         
         
         
         inline double getDomainSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._domainSize[elementIndex];
            
         }
         
         
         
         inline void setDomainSize(int elementIndex, const double& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._domainSize[elementIndex]= domainSize;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._plotName;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._plotName = (plotName);
         }
         
         
         
         inline int getPlotName(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<32);
            return _persistentRecords._plotName[elementIndex];
            
         }
         
         
         
         inline void setPlotName(int elementIndex, const int& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<32);
            _persistentRecords._plotName[elementIndex]= plotName;
            
         }
         
         
         
         inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._plotNumber;
         }
         
         
         
         inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._plotNumber = plotNumber;
         }
         
         
         
         inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._startMaximumGlobalTimeInterval;
         }
         
         
         
         inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
         }
         
         
         
         inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._endMaximumGlobalTimeInterval;
         }
         
         
         
         inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
         }
         
         
         
         inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._startMinimumGlobalTimeInterval;
         }
         
         
         
         inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
         }
         
         
         
         inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._endMinimumGlobalTimeInterval;
         }
         
         
         
         inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
         }
         
         
         
         inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._minimalTimestep;
         }
         
         
         
         inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._minimalTimestep = minimalTimestep;
         }
         
         
         
         inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._totalNumberOfCellUpdates;
         }
         
         
         
         inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._totalNumberOfCellUpdates = totalNumberOfCellUpdates;
         }
         
         
         
         inline bool getReduceReductions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._reduceReductions;
         }
         
         
         
         inline void setReduceReductions(const bool& reduceReductions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._reduceReductions = reduceReductions;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._minMeshWidth;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._minMeshWidth = (minMeshWidth);
         }
         
         
         
         inline double getMinMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._minMeshWidth[elementIndex];
            
         }
         
         
         
         inline void setMinMeshWidth(int elementIndex, const double& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._minMeshWidth[elementIndex]= minMeshWidth;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._maxMeshWidth;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._maxMeshWidth = (maxMeshWidth);
         }
         
         
         
         inline double getMaxMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._maxMeshWidth[elementIndex];
            
         }
         
         
         
         inline void setMaxMeshWidth(int elementIndex, const double& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._maxMeshWidth[elementIndex]= maxMeshWidth;
            
         }
         
         
         
         inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfInnerVertices;
         }
         
         
         
         inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfInnerVertices = numberOfInnerVertices;
         }
         
         
         
         inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfBoundaryVertices;
         }
         
         
         
         inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfBoundaryVertices = numberOfBoundaryVertices;
         }
         
         
         
         inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfOuterVertices;
         }
         
         
         
         inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfOuterVertices = numberOfOuterVertices;
         }
         
         
         
         inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfInnerCells;
         }
         
         
         
         inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfInnerCells = numberOfInnerCells;
         }
         
         
         
         inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfOuterCells;
         }
         
         
         
         inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfOuterCells = numberOfOuterCells;
         }
         
         
         
         inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfInnerLeafVertices;
         }
         
         
         
         inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfInnerLeafVertices = numberOfInnerLeafVertices;
         }
         
         
         
         inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfBoundaryLeafVertices;
         }
         
         
         
         inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
         }
         
         
         
         inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfOuterLeafVertices;
         }
         
         
         
         inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfOuterLeafVertices = numberOfOuterLeafVertices;
         }
         
         
         
         inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfInnerLeafCells;
         }
         
         
         
         inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfInnerLeafCells = numberOfInnerLeafCells;
         }
         
         
         
         inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfOuterLeafCells;
         }
         
         
         
         inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfOuterLeafCells = numberOfOuterLeafCells;
         }
         
         
         
         inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._maxLevel;
         }
         
         
         
         inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._maxLevel = maxLevel;
         }
         
         
         
         inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasRefined;
         }
         
         
         
         inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasRefined = hasRefined;
         }
         
         
         
         inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasTriggeredRefinementForNextIteration;
         }
         
         
         
         inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasTriggeredRefinementForNextIteration = hasTriggeredRefinementForNextIteration;
         }
         
         
         
         inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasErased;
         }
         
         
         
         inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasErased = hasErased;
         }
         
         
         
         inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasTriggeredEraseForNextIteration;
         }
         
         
         
         inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasTriggeredEraseForNextIteration = hasTriggeredEraseForNextIteration;
         }
         
         
         
         inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasChangedVertexOrCellState;
         }
         
         
         
         inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasChangedVertexOrCellState = hasChangedVertexOrCellState;
         }
         
         
         
         inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isTraversalInverted;
         }
         
         
         
         inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isTraversalInverted = isTraversalInverted;
         }
         
         
         
         inline bool getReduceStateAndCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._reduceStateAndCell;
         }
         
         
         
         inline void setReduceStateAndCell(const bool& reduceStateAndCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._reduceStateAndCell = reduceStateAndCell;
         }
         
         
         
         inline bool getCouldNotEraseDueToDecompositionFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._couldNotEraseDueToDecompositionFlag;
         }
         
         
         
         inline void setCouldNotEraseDueToDecompositionFlag(const bool& couldNotEraseDueToDecompositionFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._couldNotEraseDueToDecompositionFlag = couldNotEraseDueToDecompositionFlag;
         }
         
         
         
         inline bool getSubWorkerIsInvolvedInJoinOrFork() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._subWorkerIsInvolvedInJoinOrFork;
         }
         
         
         
         inline void setSubWorkerIsInvolvedInJoinOrFork(const bool& subWorkerIsInvolvedInJoinOrFork) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._subWorkerIsInvolvedInJoinOrFork = subWorkerIsInvolvedInJoinOrFork;
         }
         
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         StatePacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
            int _senderDestinationRank;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            int getSenderRank() const;
            
      #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   12/06/2014 07:41
       */
      class peanoclaw::records::StatePacked { 
         
         public:
            
            struct PersistentRecords {
               tarch::la::Vector<DIMENSIONS,double> _initialMaximalSubgridSize;
               tarch::la::Vector<DIMENSIONS,int> _defaultSubdivisionFactor;
               double _initialTimestepSize;
               double _globalTimestepEndTime;
               tarch::la::Vector<DIMENSIONS,double> _domainOffset;
               tarch::la::Vector<DIMENSIONS,double> _domainSize;
               tarch::la::Vector<32,int> _plotName;
               int _plotNumber;
               double _startMaximumGlobalTimeInterval;
               double _endMaximumGlobalTimeInterval;
               double _startMinimumGlobalTimeInterval;
               double _endMinimumGlobalTimeInterval;
               double _minimalTimestep;
               double _totalNumberOfCellUpdates;
               tarch::la::Vector<DIMENSIONS,double> _minMeshWidth;
               tarch::la::Vector<DIMENSIONS,double> _maxMeshWidth;
               double _numberOfInnerVertices;
               double _numberOfBoundaryVertices;
               double _numberOfOuterVertices;
               double _numberOfInnerCells;
               double _numberOfOuterCells;
               double _numberOfInnerLeafVertices;
               double _numberOfBoundaryLeafVertices;
               double _numberOfOuterLeafVertices;
               double _numberOfInnerLeafCells;
               double _numberOfOuterLeafCells;
               int _maxLevel;
               bool _isTraversalInverted;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isInitializing	| startbit 0	| #bits 1
                |  isRefinementCriterionEnabled	| startbit 1	| #bits 1
                |  unknownsPerSubcell	| startbit 2	| #bits 3
                |  numberOfParametersWithoutGhostlayerPerSubcell	| startbit 5	| #bits 4
                |  numberOfParametersWithGhostlayerPerSubcell	| startbit 9	| #bits 4
                |  defaultGhostWidthLayer	| startbit 13	| #bits 3
                |  useDimensionalSplittingExtrapolation	| startbit 16	| #bits 1
                |  allPatchesEvolvedToGlobalTimestep	| startbit 17	| #bits 1
                |  reduceReductions	| startbit 18	| #bits 1
                |  hasRefined	| startbit 19	| #bits 1
                |  hasTriggeredRefinementForNextIteration	| startbit 20	| #bits 1
                |  hasErased	| startbit 21	| #bits 1
                |  hasTriggeredEraseForNextIteration	| startbit 22	| #bits 1
                |  hasChangedVertexOrCellState	| startbit 23	| #bits 1
                |  reduceStateAndCell	| startbit 24	| #bits 1
                |  couldNotEraseDueToDecompositionFlag	| startbit 25	| #bits 1
                |  subWorkerIsInvolvedInJoinOrFork	| startbit 26	| #bits 1
                */
               int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const bool& reduceReductions, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted, const bool& reduceStateAndCell, const bool& couldNotEraseDueToDecompositionFlag, const bool& subWorkerIsInvolvedInJoinOrFork);
               
               
               inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isInitializing ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (1);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (1);
   _packedRecords0 = static_cast<int>( isRefinementCriterionEnabled ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((unknownsPerSubcell >= 0 && unknownsPerSubcell <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | unknownsPerSubcell << (2));
               }
               
               
               
               inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
               }
               
               
               
               inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((numberOfParametersWithoutGhostlayerPerSubcell >= 0 && numberOfParametersWithoutGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | numberOfParametersWithoutGhostlayerPerSubcell << (5));
               }
               
               
               
               inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (9));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
               }
               
               
               
               inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((numberOfParametersWithGhostlayerPerSubcell >= 0 && numberOfParametersWithGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | numberOfParametersWithGhostlayerPerSubcell << (9));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _initialMaximalSubgridSize;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _initialMaximalSubgridSize = (initialMaximalSubgridSize);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _defaultSubdivisionFactor;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _defaultSubdivisionFactor = (defaultSubdivisionFactor);
               }
               
               
               
               inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
               }
               
               
               
               inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((defaultGhostWidthLayer >= 0 && defaultGhostWidthLayer <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | defaultGhostWidthLayer << (13));
               }
               
               
               
               inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _initialTimestepSize;
               }
               
               
               
               inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _initialTimestepSize = initialTimestepSize;
               }
               
               
               
               inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (16);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (16);
   _packedRecords0 = static_cast<int>( useDimensionalSplittingExtrapolation ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _globalTimestepEndTime;
               }
               
               
               
               inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _globalTimestepEndTime = globalTimestepEndTime;
               }
               
               
               
               inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (17);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (17);
   _packedRecords0 = static_cast<int>( allPatchesEvolvedToGlobalTimestep ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _domainOffset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _domainOffset = (domainOffset);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _domainSize;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _domainSize = (domainSize);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _plotName;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _plotName = (plotName);
               }
               
               
               
               inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _plotNumber;
               }
               
               
               
               inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _plotNumber = plotNumber;
               }
               
               
               
               inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _startMaximumGlobalTimeInterval;
               }
               
               
               
               inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
               }
               
               
               
               inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _endMaximumGlobalTimeInterval;
               }
               
               
               
               inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
               }
               
               
               
               inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _startMinimumGlobalTimeInterval;
               }
               
               
               
               inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
               }
               
               
               
               inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _endMinimumGlobalTimeInterval;
               }
               
               
               
               inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
               }
               
               
               
               inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _minimalTimestep;
               }
               
               
               
               inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _minimalTimestep = minimalTimestep;
               }
               
               
               
               inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _totalNumberOfCellUpdates;
               }
               
               
               
               inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _totalNumberOfCellUpdates = totalNumberOfCellUpdates;
               }
               
               
               
               inline bool getReduceReductions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (18);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setReduceReductions(const bool& reduceReductions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (18);
   _packedRecords0 = static_cast<int>( reduceReductions ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _minMeshWidth;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _minMeshWidth = (minMeshWidth);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _maxMeshWidth;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _maxMeshWidth = (maxMeshWidth);
               }
               
               
               
               inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfInnerVertices;
               }
               
               
               
               inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfInnerVertices = numberOfInnerVertices;
               }
               
               
               
               inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfBoundaryVertices;
               }
               
               
               
               inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfBoundaryVertices = numberOfBoundaryVertices;
               }
               
               
               
               inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfOuterVertices;
               }
               
               
               
               inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfOuterVertices = numberOfOuterVertices;
               }
               
               
               
               inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfInnerCells;
               }
               
               
               
               inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfInnerCells = numberOfInnerCells;
               }
               
               
               
               inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfOuterCells;
               }
               
               
               
               inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfOuterCells = numberOfOuterCells;
               }
               
               
               
               inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfInnerLeafVertices;
               }
               
               
               
               inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfInnerLeafVertices = numberOfInnerLeafVertices;
               }
               
               
               
               inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfBoundaryLeafVertices;
               }
               
               
               
               inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
               }
               
               
               
               inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfOuterLeafVertices;
               }
               
               
               
               inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfOuterLeafVertices = numberOfOuterLeafVertices;
               }
               
               
               
               inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfInnerLeafCells;
               }
               
               
               
               inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfInnerLeafCells = numberOfInnerLeafCells;
               }
               
               
               
               inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfOuterLeafCells;
               }
               
               
               
               inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfOuterLeafCells = numberOfOuterLeafCells;
               }
               
               
               
               inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _maxLevel;
               }
               
               
               
               inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _maxLevel = maxLevel;
               }
               
               
               
               inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (19);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (19);
   _packedRecords0 = static_cast<int>( hasRefined ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (20);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (20);
   _packedRecords0 = static_cast<int>( hasTriggeredRefinementForNextIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (21);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (21);
   _packedRecords0 = static_cast<int>( hasErased ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (22);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (22);
   _packedRecords0 = static_cast<int>( hasTriggeredEraseForNextIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (23);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (23);
   _packedRecords0 = static_cast<int>( hasChangedVertexOrCellState ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _isTraversalInverted;
               }
               
               
               
               inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _isTraversalInverted = isTraversalInverted;
               }
               
               
               
               inline bool getReduceStateAndCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (24);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setReduceStateAndCell(const bool& reduceStateAndCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (24);
   _packedRecords0 = static_cast<int>( reduceStateAndCell ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getCouldNotEraseDueToDecompositionFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (25);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setCouldNotEraseDueToDecompositionFlag(const bool& couldNotEraseDueToDecompositionFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (25);
   _packedRecords0 = static_cast<int>( couldNotEraseDueToDecompositionFlag ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getSubWorkerIsInvolvedInJoinOrFork() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (26);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setSubWorkerIsInvolvedInJoinOrFork(const bool& subWorkerIsInvolvedInJoinOrFork) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  int mask = 1 << (26);
   _packedRecords0 = static_cast<int>( subWorkerIsInvolvedInJoinOrFork ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            StatePacked();
            
            /**
             * Generated
             */
            StatePacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            StatePacked(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const bool& reduceReductions, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted, const bool& reduceStateAndCell, const bool& couldNotEraseDueToDecompositionFlag, const bool& subWorkerIsInvolvedInJoinOrFork);
            
            /**
             * Generated
             */
            virtual ~StatePacked();
            
            
            inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isInitializing ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (1);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (1);
   _persistentRecords._packedRecords0 = static_cast<int>( isRefinementCriterionEnabled ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((unknownsPerSubcell >= 0 && unknownsPerSubcell <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | unknownsPerSubcell << (2));
            }
            
            
            
            inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
            }
            
            
            
            inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((numberOfParametersWithoutGhostlayerPerSubcell >= 0 && numberOfParametersWithoutGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | numberOfParametersWithoutGhostlayerPerSubcell << (5));
            }
            
            
            
            inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (9));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
            }
            
            
            
            inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((numberOfParametersWithGhostlayerPerSubcell >= 0 && numberOfParametersWithGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | numberOfParametersWithGhostlayerPerSubcell << (9));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._initialMaximalSubgridSize;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._initialMaximalSubgridSize = (initialMaximalSubgridSize);
            }
            
            
            
            inline double getInitialMaximalSubgridSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._initialMaximalSubgridSize[elementIndex];
               
            }
            
            
            
            inline void setInitialMaximalSubgridSize(int elementIndex, const double& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._initialMaximalSubgridSize[elementIndex]= initialMaximalSubgridSize;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._defaultSubdivisionFactor;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._defaultSubdivisionFactor = (defaultSubdivisionFactor);
            }
            
            
            
            inline int getDefaultSubdivisionFactor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._defaultSubdivisionFactor[elementIndex];
               
            }
            
            
            
            inline void setDefaultSubdivisionFactor(int elementIndex, const int& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._defaultSubdivisionFactor[elementIndex]= defaultSubdivisionFactor;
               
            }
            
            
            
            inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
            }
            
            
            
            inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((defaultGhostWidthLayer >= 0 && defaultGhostWidthLayer <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | defaultGhostWidthLayer << (13));
            }
            
            
            
            inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._initialTimestepSize;
            }
            
            
            
            inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._initialTimestepSize = initialTimestepSize;
            }
            
            
            
            inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (16);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (16);
   _persistentRecords._packedRecords0 = static_cast<int>( useDimensionalSplittingExtrapolation ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._globalTimestepEndTime;
            }
            
            
            
            inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._globalTimestepEndTime = globalTimestepEndTime;
            }
            
            
            
            inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (17);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (17);
   _persistentRecords._packedRecords0 = static_cast<int>( allPatchesEvolvedToGlobalTimestep ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._domainOffset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._domainOffset = (domainOffset);
            }
            
            
            
            inline double getDomainOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._domainOffset[elementIndex];
               
            }
            
            
            
            inline void setDomainOffset(int elementIndex, const double& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._domainOffset[elementIndex]= domainOffset;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._domainSize;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._domainSize = (domainSize);
            }
            
            
            
            inline double getDomainSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._domainSize[elementIndex];
               
            }
            
            
            
            inline void setDomainSize(int elementIndex, const double& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._domainSize[elementIndex]= domainSize;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._plotName;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._plotName = (plotName);
            }
            
            
            
            inline int getPlotName(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<32);
               return _persistentRecords._plotName[elementIndex];
               
            }
            
            
            
            inline void setPlotName(int elementIndex, const int& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<32);
               _persistentRecords._plotName[elementIndex]= plotName;
               
            }
            
            
            
            inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._plotNumber;
            }
            
            
            
            inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._plotNumber = plotNumber;
            }
            
            
            
            inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._startMaximumGlobalTimeInterval;
            }
            
            
            
            inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
            }
            
            
            
            inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._endMaximumGlobalTimeInterval;
            }
            
            
            
            inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
            }
            
            
            
            inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._startMinimumGlobalTimeInterval;
            }
            
            
            
            inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
            }
            
            
            
            inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._endMinimumGlobalTimeInterval;
            }
            
            
            
            inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
            }
            
            
            
            inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._minimalTimestep;
            }
            
            
            
            inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._minimalTimestep = minimalTimestep;
            }
            
            
            
            inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._totalNumberOfCellUpdates;
            }
            
            
            
            inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._totalNumberOfCellUpdates = totalNumberOfCellUpdates;
            }
            
            
            
            inline bool getReduceReductions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (18);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setReduceReductions(const bool& reduceReductions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (18);
   _persistentRecords._packedRecords0 = static_cast<int>( reduceReductions ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._minMeshWidth;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._minMeshWidth = (minMeshWidth);
            }
            
            
            
            inline double getMinMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._minMeshWidth[elementIndex];
               
            }
            
            
            
            inline void setMinMeshWidth(int elementIndex, const double& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._minMeshWidth[elementIndex]= minMeshWidth;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._maxMeshWidth;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._maxMeshWidth = (maxMeshWidth);
            }
            
            
            
            inline double getMaxMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._maxMeshWidth[elementIndex];
               
            }
            
            
            
            inline void setMaxMeshWidth(int elementIndex, const double& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._maxMeshWidth[elementIndex]= maxMeshWidth;
               
            }
            
            
            
            inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfInnerVertices;
            }
            
            
            
            inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfInnerVertices = numberOfInnerVertices;
            }
            
            
            
            inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfBoundaryVertices;
            }
            
            
            
            inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfBoundaryVertices = numberOfBoundaryVertices;
            }
            
            
            
            inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfOuterVertices;
            }
            
            
            
            inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfOuterVertices = numberOfOuterVertices;
            }
            
            
            
            inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfInnerCells;
            }
            
            
            
            inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfInnerCells = numberOfInnerCells;
            }
            
            
            
            inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfOuterCells;
            }
            
            
            
            inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfOuterCells = numberOfOuterCells;
            }
            
            
            
            inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfInnerLeafVertices;
            }
            
            
            
            inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfInnerLeafVertices = numberOfInnerLeafVertices;
            }
            
            
            
            inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfBoundaryLeafVertices;
            }
            
            
            
            inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
            }
            
            
            
            inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfOuterLeafVertices;
            }
            
            
            
            inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfOuterLeafVertices = numberOfOuterLeafVertices;
            }
            
            
            
            inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfInnerLeafCells;
            }
            
            
            
            inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfInnerLeafCells = numberOfInnerLeafCells;
            }
            
            
            
            inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfOuterLeafCells;
            }
            
            
            
            inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfOuterLeafCells = numberOfOuterLeafCells;
            }
            
            
            
            inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._maxLevel;
            }
            
            
            
            inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._maxLevel = maxLevel;
            }
            
            
            
            inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (19);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (19);
   _persistentRecords._packedRecords0 = static_cast<int>( hasRefined ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (20);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (20);
   _persistentRecords._packedRecords0 = static_cast<int>( hasTriggeredRefinementForNextIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (21);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (21);
   _persistentRecords._packedRecords0 = static_cast<int>( hasErased ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (22);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (22);
   _persistentRecords._packedRecords0 = static_cast<int>( hasTriggeredEraseForNextIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (23);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (23);
   _persistentRecords._packedRecords0 = static_cast<int>( hasChangedVertexOrCellState ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._isTraversalInverted;
            }
            
            
            
            inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._isTraversalInverted = isTraversalInverted;
            }
            
            
            
            inline bool getReduceStateAndCell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (24);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setReduceStateAndCell(const bool& reduceStateAndCell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (24);
   _persistentRecords._packedRecords0 = static_cast<int>( reduceStateAndCell ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getCouldNotEraseDueToDecompositionFlag() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (25);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setCouldNotEraseDueToDecompositionFlag(const bool& couldNotEraseDueToDecompositionFlag) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (25);
   _persistentRecords._packedRecords0 = static_cast<int>( couldNotEraseDueToDecompositionFlag ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getSubWorkerIsInvolvedInJoinOrFork() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (26);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setSubWorkerIsInvolvedInJoinOrFork(const bool& subWorkerIsInvolvedInJoinOrFork) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               int mask = 1 << (26);
   _persistentRecords._packedRecords0 = static_cast<int>( subWorkerIsInvolvedInJoinOrFork ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            State convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               
         #endif
            
         };
         
         #ifdef PackedRecords
         #pragma pack (pop)
         #endif
         
         
         
      #elif !defined(Parallel)
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 09-02-2014 14:40
          *
          * @date   12/06/2014 07:41
          */
         class peanoclaw::records::State { 
            
            public:
               
               typedef peanoclaw::records::StatePacked Packed;
               
               struct PersistentRecords {
                  bool _isInitializing;
                  bool _isRefinementCriterionEnabled;
                  int _unknownsPerSubcell;
                  int _numberOfParametersWithoutGhostlayerPerSubcell;
                  int _numberOfParametersWithGhostlayerPerSubcell;
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,double> _initialMaximalSubgridSize __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,double> _initialMaximalSubgridSize;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,int> _defaultSubdivisionFactor __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,int> _defaultSubdivisionFactor;
                  #endif
                  int _defaultGhostWidthLayer;
                  double _initialTimestepSize;
                  bool _useDimensionalSplittingExtrapolation;
                  double _globalTimestepEndTime;
                  bool _allPatchesEvolvedToGlobalTimestep;
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,double> _domainOffset __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,double> _domainOffset;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,double> _domainSize __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,double> _domainSize;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<32,int> _plotName __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<32,int> _plotName;
                  #endif
                  int _plotNumber;
                  double _startMaximumGlobalTimeInterval;
                  double _endMaximumGlobalTimeInterval;
                  double _startMinimumGlobalTimeInterval;
                  double _endMinimumGlobalTimeInterval;
                  double _minimalTimestep;
                  double _totalNumberOfCellUpdates;
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,double> _minMeshWidth __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,double> _minMeshWidth;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,double> _maxMeshWidth __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,double> _maxMeshWidth;
                  #endif
                  double _numberOfInnerVertices;
                  double _numberOfBoundaryVertices;
                  double _numberOfOuterVertices;
                  double _numberOfInnerCells;
                  double _numberOfOuterCells;
                  double _numberOfInnerLeafVertices;
                  double _numberOfBoundaryLeafVertices;
                  double _numberOfOuterLeafVertices;
                  double _numberOfInnerLeafCells;
                  double _numberOfOuterLeafCells;
                  int _maxLevel;
                  bool _hasRefined;
                  bool _hasTriggeredRefinementForNextIteration;
                  bool _hasErased;
                  bool _hasTriggeredEraseForNextIteration;
                  bool _hasChangedVertexOrCellState;
                  bool _isTraversalInverted;
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted);
                  
                  
                  inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isInitializing;
                  }
                  
                  
                  
                  inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isInitializing = isInitializing;
                  }
                  
                  
                  
                  inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isRefinementCriterionEnabled;
                  }
                  
                  
                  
                  inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isRefinementCriterionEnabled = isRefinementCriterionEnabled;
                  }
                  
                  
                  
                  inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _unknownsPerSubcell;
                  }
                  
                  
                  
                  inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _unknownsPerSubcell = unknownsPerSubcell;
                  }
                  
                  
                  
                  inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfParametersWithoutGhostlayerPerSubcell;
                  }
                  
                  
                  
                  inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfParametersWithoutGhostlayerPerSubcell = numberOfParametersWithoutGhostlayerPerSubcell;
                  }
                  
                  
                  
                  inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfParametersWithGhostlayerPerSubcell;
                  }
                  
                  
                  
                  inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfParametersWithGhostlayerPerSubcell = numberOfParametersWithGhostlayerPerSubcell;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _initialMaximalSubgridSize;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _initialMaximalSubgridSize = (initialMaximalSubgridSize);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _defaultSubdivisionFactor;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _defaultSubdivisionFactor = (defaultSubdivisionFactor);
                  }
                  
                  
                  
                  inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _defaultGhostWidthLayer;
                  }
                  
                  
                  
                  inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _defaultGhostWidthLayer = defaultGhostWidthLayer;
                  }
                  
                  
                  
                  inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _initialTimestepSize;
                  }
                  
                  
                  
                  inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _initialTimestepSize = initialTimestepSize;
                  }
                  
                  
                  
                  inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _useDimensionalSplittingExtrapolation;
                  }
                  
                  
                  
                  inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _useDimensionalSplittingExtrapolation = useDimensionalSplittingExtrapolation;
                  }
                  
                  
                  
                  inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _globalTimestepEndTime;
                  }
                  
                  
                  
                  inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _globalTimestepEndTime = globalTimestepEndTime;
                  }
                  
                  
                  
                  inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _allPatchesEvolvedToGlobalTimestep;
                  }
                  
                  
                  
                  inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _allPatchesEvolvedToGlobalTimestep = allPatchesEvolvedToGlobalTimestep;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _domainOffset;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _domainOffset = (domainOffset);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _domainSize;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _domainSize = (domainSize);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _plotName;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _plotName = (plotName);
                  }
                  
                  
                  
                  inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _plotNumber;
                  }
                  
                  
                  
                  inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _plotNumber = plotNumber;
                  }
                  
                  
                  
                  inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _startMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _endMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _startMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _endMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _minimalTimestep;
                  }
                  
                  
                  
                  inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _minimalTimestep = minimalTimestep;
                  }
                  
                  
                  
                  inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _totalNumberOfCellUpdates;
                  }
                  
                  
                  
                  inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _totalNumberOfCellUpdates = totalNumberOfCellUpdates;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _minMeshWidth;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _minMeshWidth = (minMeshWidth);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _maxMeshWidth;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _maxMeshWidth = (maxMeshWidth);
                  }
                  
                  
                  
                  inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfInnerVertices;
                  }
                  
                  
                  
                  inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfInnerVertices = numberOfInnerVertices;
                  }
                  
                  
                  
                  inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfBoundaryVertices;
                  }
                  
                  
                  
                  inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfBoundaryVertices = numberOfBoundaryVertices;
                  }
                  
                  
                  
                  inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfOuterVertices;
                  }
                  
                  
                  
                  inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfOuterVertices = numberOfOuterVertices;
                  }
                  
                  
                  
                  inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfInnerCells;
                  }
                  
                  
                  
                  inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfInnerCells = numberOfInnerCells;
                  }
                  
                  
                  
                  inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfOuterCells;
                  }
                  
                  
                  
                  inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfOuterCells = numberOfOuterCells;
                  }
                  
                  
                  
                  inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfInnerLeafVertices;
                  }
                  
                  
                  
                  inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfInnerLeafVertices = numberOfInnerLeafVertices;
                  }
                  
                  
                  
                  inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfBoundaryLeafVertices;
                  }
                  
                  
                  
                  inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
                  }
                  
                  
                  
                  inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfOuterLeafVertices;
                  }
                  
                  
                  
                  inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfOuterLeafVertices = numberOfOuterLeafVertices;
                  }
                  
                  
                  
                  inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfInnerLeafCells;
                  }
                  
                  
                  
                  inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfInnerLeafCells = numberOfInnerLeafCells;
                  }
                  
                  
                  
                  inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfOuterLeafCells;
                  }
                  
                  
                  
                  inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfOuterLeafCells = numberOfOuterLeafCells;
                  }
                  
                  
                  
                  inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _maxLevel;
                  }
                  
                  
                  
                  inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _maxLevel = maxLevel;
                  }
                  
                  
                  
                  inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _hasRefined;
                  }
                  
                  
                  
                  inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _hasRefined = hasRefined;
                  }
                  
                  
                  
                  inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _hasTriggeredRefinementForNextIteration;
                  }
                  
                  
                  
                  inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _hasTriggeredRefinementForNextIteration = hasTriggeredRefinementForNextIteration;
                  }
                  
                  
                  
                  inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _hasErased;
                  }
                  
                  
                  
                  inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _hasErased = hasErased;
                  }
                  
                  
                  
                  inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _hasTriggeredEraseForNextIteration;
                  }
                  
                  
                  
                  inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _hasTriggeredEraseForNextIteration = hasTriggeredEraseForNextIteration;
                  }
                  
                  
                  
                  inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _hasChangedVertexOrCellState;
                  }
                  
                  
                  
                  inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _hasChangedVertexOrCellState = hasChangedVertexOrCellState;
                  }
                  
                  
                  
                  inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isTraversalInverted;
                  }
                  
                  
                  
                  inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isTraversalInverted = isTraversalInverted;
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               State();
               
               /**
                * Generated
                */
               State(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               State(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted);
               
               /**
                * Generated
                */
               virtual ~State();
               
               
               inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isInitializing;
               }
               
               
               
               inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isInitializing = isInitializing;
               }
               
               
               
               inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isRefinementCriterionEnabled;
               }
               
               
               
               inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isRefinementCriterionEnabled = isRefinementCriterionEnabled;
               }
               
               
               
               inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._unknownsPerSubcell;
               }
               
               
               
               inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._unknownsPerSubcell = unknownsPerSubcell;
               }
               
               
               
               inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfParametersWithoutGhostlayerPerSubcell;
               }
               
               
               
               inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfParametersWithoutGhostlayerPerSubcell = numberOfParametersWithoutGhostlayerPerSubcell;
               }
               
               
               
               inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfParametersWithGhostlayerPerSubcell;
               }
               
               
               
               inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfParametersWithGhostlayerPerSubcell = numberOfParametersWithGhostlayerPerSubcell;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._initialMaximalSubgridSize;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._initialMaximalSubgridSize = (initialMaximalSubgridSize);
               }
               
               
               
               inline double getInitialMaximalSubgridSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._initialMaximalSubgridSize[elementIndex];
                  
               }
               
               
               
               inline void setInitialMaximalSubgridSize(int elementIndex, const double& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._initialMaximalSubgridSize[elementIndex]= initialMaximalSubgridSize;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._defaultSubdivisionFactor;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._defaultSubdivisionFactor = (defaultSubdivisionFactor);
               }
               
               
               
               inline int getDefaultSubdivisionFactor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._defaultSubdivisionFactor[elementIndex];
                  
               }
               
               
               
               inline void setDefaultSubdivisionFactor(int elementIndex, const int& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._defaultSubdivisionFactor[elementIndex]= defaultSubdivisionFactor;
                  
               }
               
               
               
               inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._defaultGhostWidthLayer;
               }
               
               
               
               inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._defaultGhostWidthLayer = defaultGhostWidthLayer;
               }
               
               
               
               inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._initialTimestepSize;
               }
               
               
               
               inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._initialTimestepSize = initialTimestepSize;
               }
               
               
               
               inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._useDimensionalSplittingExtrapolation;
               }
               
               
               
               inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._useDimensionalSplittingExtrapolation = useDimensionalSplittingExtrapolation;
               }
               
               
               
               inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._globalTimestepEndTime;
               }
               
               
               
               inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._globalTimestepEndTime = globalTimestepEndTime;
               }
               
               
               
               inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._allPatchesEvolvedToGlobalTimestep;
               }
               
               
               
               inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._allPatchesEvolvedToGlobalTimestep = allPatchesEvolvedToGlobalTimestep;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._domainOffset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._domainOffset = (domainOffset);
               }
               
               
               
               inline double getDomainOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._domainOffset[elementIndex];
                  
               }
               
               
               
               inline void setDomainOffset(int elementIndex, const double& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._domainOffset[elementIndex]= domainOffset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._domainSize;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._domainSize = (domainSize);
               }
               
               
               
               inline double getDomainSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._domainSize[elementIndex];
                  
               }
               
               
               
               inline void setDomainSize(int elementIndex, const double& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._domainSize[elementIndex]= domainSize;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._plotName;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._plotName = (plotName);
               }
               
               
               
               inline int getPlotName(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<32);
                  return _persistentRecords._plotName[elementIndex];
                  
               }
               
               
               
               inline void setPlotName(int elementIndex, const int& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<32);
                  _persistentRecords._plotName[elementIndex]= plotName;
                  
               }
               
               
               
               inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._plotNumber;
               }
               
               
               
               inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._plotNumber = plotNumber;
               }
               
               
               
               inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._startMaximumGlobalTimeInterval;
               }
               
               
               
               inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
               }
               
               
               
               inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._endMaximumGlobalTimeInterval;
               }
               
               
               
               inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
               }
               
               
               
               inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._startMinimumGlobalTimeInterval;
               }
               
               
               
               inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
               }
               
               
               
               inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._endMinimumGlobalTimeInterval;
               }
               
               
               
               inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
               }
               
               
               
               inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._minimalTimestep;
               }
               
               
               
               inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._minimalTimestep = minimalTimestep;
               }
               
               
               
               inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._totalNumberOfCellUpdates;
               }
               
               
               
               inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._totalNumberOfCellUpdates = totalNumberOfCellUpdates;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._minMeshWidth;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._minMeshWidth = (minMeshWidth);
               }
               
               
               
               inline double getMinMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._minMeshWidth[elementIndex];
                  
               }
               
               
               
               inline void setMinMeshWidth(int elementIndex, const double& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._minMeshWidth[elementIndex]= minMeshWidth;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._maxMeshWidth;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._maxMeshWidth = (maxMeshWidth);
               }
               
               
               
               inline double getMaxMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._maxMeshWidth[elementIndex];
                  
               }
               
               
               
               inline void setMaxMeshWidth(int elementIndex, const double& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._maxMeshWidth[elementIndex]= maxMeshWidth;
                  
               }
               
               
               
               inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfInnerVertices;
               }
               
               
               
               inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfInnerVertices = numberOfInnerVertices;
               }
               
               
               
               inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfBoundaryVertices;
               }
               
               
               
               inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfBoundaryVertices = numberOfBoundaryVertices;
               }
               
               
               
               inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfOuterVertices;
               }
               
               
               
               inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfOuterVertices = numberOfOuterVertices;
               }
               
               
               
               inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfInnerCells;
               }
               
               
               
               inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfInnerCells = numberOfInnerCells;
               }
               
               
               
               inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfOuterCells;
               }
               
               
               
               inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfOuterCells = numberOfOuterCells;
               }
               
               
               
               inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfInnerLeafVertices;
               }
               
               
               
               inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfInnerLeafVertices = numberOfInnerLeafVertices;
               }
               
               
               
               inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfBoundaryLeafVertices;
               }
               
               
               
               inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
               }
               
               
               
               inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfOuterLeafVertices;
               }
               
               
               
               inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfOuterLeafVertices = numberOfOuterLeafVertices;
               }
               
               
               
               inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfInnerLeafCells;
               }
               
               
               
               inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfInnerLeafCells = numberOfInnerLeafCells;
               }
               
               
               
               inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfOuterLeafCells;
               }
               
               
               
               inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfOuterLeafCells = numberOfOuterLeafCells;
               }
               
               
               
               inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._maxLevel;
               }
               
               
               
               inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._maxLevel = maxLevel;
               }
               
               
               
               inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasRefined;
               }
               
               
               
               inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasRefined = hasRefined;
               }
               
               
               
               inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasTriggeredRefinementForNextIteration;
               }
               
               
               
               inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasTriggeredRefinementForNextIteration = hasTriggeredRefinementForNextIteration;
               }
               
               
               
               inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasErased;
               }
               
               
               
               inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasErased = hasErased;
               }
               
               
               
               inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasTriggeredEraseForNextIteration;
               }
               
               
               
               inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasTriggeredEraseForNextIteration = hasTriggeredEraseForNextIteration;
               }
               
               
               
               inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._hasChangedVertexOrCellState;
               }
               
               
               
               inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._hasChangedVertexOrCellState = hasChangedVertexOrCellState;
               }
               
               
               
               inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isTraversalInverted;
               }
               
               
               
               inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isTraversalInverted = isTraversalInverted;
               }
               
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               StatePacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 09-02-2014 14:40
             *
             * @date   12/06/2014 07:41
             */
            class peanoclaw::records::StatePacked { 
               
               public:
                  
                  struct PersistentRecords {
                     tarch::la::Vector<DIMENSIONS,double> _initialMaximalSubgridSize;
                     tarch::la::Vector<DIMENSIONS,int> _defaultSubdivisionFactor;
                     double _initialTimestepSize;
                     double _globalTimestepEndTime;
                     tarch::la::Vector<DIMENSIONS,double> _domainOffset;
                     tarch::la::Vector<DIMENSIONS,double> _domainSize;
                     tarch::la::Vector<32,int> _plotName;
                     int _plotNumber;
                     double _startMaximumGlobalTimeInterval;
                     double _endMaximumGlobalTimeInterval;
                     double _startMinimumGlobalTimeInterval;
                     double _endMinimumGlobalTimeInterval;
                     double _minimalTimestep;
                     double _totalNumberOfCellUpdates;
                     tarch::la::Vector<DIMENSIONS,double> _minMeshWidth;
                     tarch::la::Vector<DIMENSIONS,double> _maxMeshWidth;
                     double _numberOfInnerVertices;
                     double _numberOfBoundaryVertices;
                     double _numberOfOuterVertices;
                     double _numberOfInnerCells;
                     double _numberOfOuterCells;
                     double _numberOfInnerLeafVertices;
                     double _numberOfBoundaryLeafVertices;
                     double _numberOfOuterLeafVertices;
                     double _numberOfInnerLeafCells;
                     double _numberOfOuterLeafCells;
                     int _maxLevel;
                     bool _isTraversalInverted;
                     
                     /** mapping of records:
                     || Member 	|| startbit 	|| length
                      |  isInitializing	| startbit 0	| #bits 1
                      |  isRefinementCriterionEnabled	| startbit 1	| #bits 1
                      |  unknownsPerSubcell	| startbit 2	| #bits 3
                      |  numberOfParametersWithoutGhostlayerPerSubcell	| startbit 5	| #bits 4
                      |  numberOfParametersWithGhostlayerPerSubcell	| startbit 9	| #bits 4
                      |  defaultGhostWidthLayer	| startbit 13	| #bits 3
                      |  useDimensionalSplittingExtrapolation	| startbit 16	| #bits 1
                      |  allPatchesEvolvedToGlobalTimestep	| startbit 17	| #bits 1
                      |  hasRefined	| startbit 18	| #bits 1
                      |  hasTriggeredRefinementForNextIteration	| startbit 19	| #bits 1
                      |  hasErased	| startbit 20	| #bits 1
                      |  hasTriggeredEraseForNextIteration	| startbit 21	| #bits 1
                      |  hasChangedVertexOrCellState	| startbit 22	| #bits 1
                      */
                     int _packedRecords0;
                     
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted);
                     
                     
                     inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (0);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (0);
   _packedRecords0 = static_cast<int>( isInitializing ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (1);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (1);
   _packedRecords0 = static_cast<int>( isRefinementCriterionEnabled ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
                     }
                     
                     
                     
                     inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((unknownsPerSubcell >= 0 && unknownsPerSubcell <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | unknownsPerSubcell << (2));
                     }
                     
                     
                     
                     inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
                     }
                     
                     
                     
                     inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((numberOfParametersWithoutGhostlayerPerSubcell >= 0 && numberOfParametersWithoutGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | numberOfParametersWithoutGhostlayerPerSubcell << (5));
                     }
                     
                     
                     
                     inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (9));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
                     }
                     
                     
                     
                     inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((numberOfParametersWithGhostlayerPerSubcell >= 0 && numberOfParametersWithGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | numberOfParametersWithGhostlayerPerSubcell << (9));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _initialMaximalSubgridSize;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _initialMaximalSubgridSize = (initialMaximalSubgridSize);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _defaultSubdivisionFactor;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _defaultSubdivisionFactor = (defaultSubdivisionFactor);
                     }
                     
                     
                     
                     inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
                     }
                     
                     
                     
                     inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((defaultGhostWidthLayer >= 0 && defaultGhostWidthLayer <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _packedRecords0 = static_cast<int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<int>(_packedRecords0 | defaultGhostWidthLayer << (13));
                     }
                     
                     
                     
                     inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _initialTimestepSize;
                     }
                     
                     
                     
                     inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _initialTimestepSize = initialTimestepSize;
                     }
                     
                     
                     
                     inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (16);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (16);
   _packedRecords0 = static_cast<int>( useDimensionalSplittingExtrapolation ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _globalTimestepEndTime;
                     }
                     
                     
                     
                     inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _globalTimestepEndTime = globalTimestepEndTime;
                     }
                     
                     
                     
                     inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (17);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (17);
   _packedRecords0 = static_cast<int>( allPatchesEvolvedToGlobalTimestep ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _domainOffset;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _domainOffset = (domainOffset);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _domainSize;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _domainSize = (domainSize);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _plotName;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _plotName = (plotName);
                     }
                     
                     
                     
                     inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _plotNumber;
                     }
                     
                     
                     
                     inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _plotNumber = plotNumber;
                     }
                     
                     
                     
                     inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _startMaximumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _endMaximumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _startMinimumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _endMinimumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
                     }
                     
                     
                     
                     inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _minimalTimestep;
                     }
                     
                     
                     
                     inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _minimalTimestep = minimalTimestep;
                     }
                     
                     
                     
                     inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _totalNumberOfCellUpdates;
                     }
                     
                     
                     
                     inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _totalNumberOfCellUpdates = totalNumberOfCellUpdates;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _minMeshWidth;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _minMeshWidth = (minMeshWidth);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _maxMeshWidth;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _maxMeshWidth = (maxMeshWidth);
                     }
                     
                     
                     
                     inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfInnerVertices;
                     }
                     
                     
                     
                     inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfInnerVertices = numberOfInnerVertices;
                     }
                     
                     
                     
                     inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfBoundaryVertices;
                     }
                     
                     
                     
                     inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfBoundaryVertices = numberOfBoundaryVertices;
                     }
                     
                     
                     
                     inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfOuterVertices;
                     }
                     
                     
                     
                     inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfOuterVertices = numberOfOuterVertices;
                     }
                     
                     
                     
                     inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfInnerCells;
                     }
                     
                     
                     
                     inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfInnerCells = numberOfInnerCells;
                     }
                     
                     
                     
                     inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfOuterCells;
                     }
                     
                     
                     
                     inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfOuterCells = numberOfOuterCells;
                     }
                     
                     
                     
                     inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfInnerLeafVertices;
                     }
                     
                     
                     
                     inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfInnerLeafVertices = numberOfInnerLeafVertices;
                     }
                     
                     
                     
                     inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfBoundaryLeafVertices;
                     }
                     
                     
                     
                     inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
                     }
                     
                     
                     
                     inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfOuterLeafVertices;
                     }
                     
                     
                     
                     inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfOuterLeafVertices = numberOfOuterLeafVertices;
                     }
                     
                     
                     
                     inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfInnerLeafCells;
                     }
                     
                     
                     
                     inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfInnerLeafCells = numberOfInnerLeafCells;
                     }
                     
                     
                     
                     inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfOuterLeafCells;
                     }
                     
                     
                     
                     inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfOuterLeafCells = numberOfOuterLeafCells;
                     }
                     
                     
                     
                     inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _maxLevel;
                     }
                     
                     
                     
                     inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _maxLevel = maxLevel;
                     }
                     
                     
                     
                     inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (18);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (18);
   _packedRecords0 = static_cast<int>( hasRefined ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (19);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (19);
   _packedRecords0 = static_cast<int>( hasTriggeredRefinementForNextIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (20);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (20);
   _packedRecords0 = static_cast<int>( hasErased ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (21);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (21);
   _packedRecords0 = static_cast<int>( hasTriggeredEraseForNextIteration ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (22);
   int tmp = static_cast<int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        int mask = 1 << (22);
   _packedRecords0 = static_cast<int>( hasChangedVertexOrCellState ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isTraversalInverted;
                     }
                     
                     
                     
                     inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isTraversalInverted = isTraversalInverted;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  
               public:
                  /**
                   * Generated
                   */
                  StatePacked();
                  
                  /**
                   * Generated
                   */
                  StatePacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  StatePacked(const bool& isInitializing, const bool& isRefinementCriterionEnabled, const int& unknownsPerSubcell, const int& numberOfParametersWithoutGhostlayerPerSubcell, const int& numberOfParametersWithGhostlayerPerSubcell, const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize, const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor, const int& defaultGhostWidthLayer, const double& initialTimestepSize, const bool& useDimensionalSplittingExtrapolation, const double& globalTimestepEndTime, const bool& allPatchesEvolvedToGlobalTimestep, const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& domainSize, const tarch::la::Vector<32,int>& plotName, const int& plotNumber, const double& startMaximumGlobalTimeInterval, const double& endMaximumGlobalTimeInterval, const double& startMinimumGlobalTimeInterval, const double& endMinimumGlobalTimeInterval, const double& minimalTimestep, const double& totalNumberOfCellUpdates, const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth, const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth, const double& numberOfInnerVertices, const double& numberOfBoundaryVertices, const double& numberOfOuterVertices, const double& numberOfInnerCells, const double& numberOfOuterCells, const double& numberOfInnerLeafVertices, const double& numberOfBoundaryLeafVertices, const double& numberOfOuterLeafVertices, const double& numberOfInnerLeafCells, const double& numberOfOuterLeafCells, const int& maxLevel, const bool& hasRefined, const bool& hasTriggeredRefinementForNextIteration, const bool& hasErased, const bool& hasTriggeredEraseForNextIteration, const bool& hasChangedVertexOrCellState, const bool& isTraversalInverted);
                  
                  /**
                   * Generated
                   */
                  virtual ~StatePacked();
                  
                  
                  inline bool getIsInitializing() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (0);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setIsInitializing(const bool& isInitializing) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<int>( isInitializing ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getIsRefinementCriterionEnabled() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (1);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setIsRefinementCriterionEnabled(const bool& isRefinementCriterionEnabled) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (1);
   _persistentRecords._packedRecords0 = static_cast<int>( isRefinementCriterionEnabled ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline int getUnknownsPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (2));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
                  }
                  
                  
                  
                  inline void setUnknownsPerSubcell(const int& unknownsPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((unknownsPerSubcell >= 0 && unknownsPerSubcell <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (2));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | unknownsPerSubcell << (2));
                  }
                  
                  
                  
                  inline int getNumberOfParametersWithoutGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (5));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
                  }
                  
                  
                  
                  inline void setNumberOfParametersWithoutGhostlayerPerSubcell(const int& numberOfParametersWithoutGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((numberOfParametersWithoutGhostlayerPerSubcell >= 0 && numberOfParametersWithoutGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (5));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | numberOfParametersWithoutGhostlayerPerSubcell << (5));
                  }
                  
                  
                  
                  inline int getNumberOfParametersWithGhostlayerPerSubcell() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (9));
   assertion(( tmp >= 0 &&  tmp <= 15));
   return (int) tmp;
                  }
                  
                  
                  
                  inline void setNumberOfParametersWithGhostlayerPerSubcell(const int& numberOfParametersWithGhostlayerPerSubcell) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((numberOfParametersWithGhostlayerPerSubcell >= 0 && numberOfParametersWithGhostlayerPerSubcell <= 15));
   int mask =  (1 << (4)) - 1;
   mask = static_cast<int>(mask << (9));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | numberOfParametersWithGhostlayerPerSubcell << (9));
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getInitialMaximalSubgridSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._initialMaximalSubgridSize;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setInitialMaximalSubgridSize(const tarch::la::Vector<DIMENSIONS,double>& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._initialMaximalSubgridSize = (initialMaximalSubgridSize);
                  }
                  
                  
                  
                  inline double getInitialMaximalSubgridSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._initialMaximalSubgridSize[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setInitialMaximalSubgridSize(int elementIndex, const double& initialMaximalSubgridSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._initialMaximalSubgridSize[elementIndex]= initialMaximalSubgridSize;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,int> getDefaultSubdivisionFactor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._defaultSubdivisionFactor;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setDefaultSubdivisionFactor(const tarch::la::Vector<DIMENSIONS,int>& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._defaultSubdivisionFactor = (defaultSubdivisionFactor);
                  }
                  
                  
                  
                  inline int getDefaultSubdivisionFactor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._defaultSubdivisionFactor[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setDefaultSubdivisionFactor(int elementIndex, const int& defaultSubdivisionFactor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._defaultSubdivisionFactor[elementIndex]= defaultSubdivisionFactor;
                     
                  }
                  
                  
                  
                  inline int getDefaultGhostWidthLayer() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<int>(tmp >> (13));
   assertion(( tmp >= 0 &&  tmp <= 7));
   return (int) tmp;
                  }
                  
                  
                  
                  inline void setDefaultGhostWidthLayer(const int& defaultGhostWidthLayer) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((defaultGhostWidthLayer >= 0 && defaultGhostWidthLayer <= 7));
   int mask =  (1 << (3)) - 1;
   mask = static_cast<int>(mask << (13));
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<int>(_persistentRecords._packedRecords0 | defaultGhostWidthLayer << (13));
                  }
                  
                  
                  
                  inline double getInitialTimestepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._initialTimestepSize;
                  }
                  
                  
                  
                  inline void setInitialTimestepSize(const double& initialTimestepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._initialTimestepSize = initialTimestepSize;
                  }
                  
                  
                  
                  inline bool getUseDimensionalSplittingExtrapolation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (16);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setUseDimensionalSplittingExtrapolation(const bool& useDimensionalSplittingExtrapolation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (16);
   _persistentRecords._packedRecords0 = static_cast<int>( useDimensionalSplittingExtrapolation ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline double getGlobalTimestepEndTime() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._globalTimestepEndTime;
                  }
                  
                  
                  
                  inline void setGlobalTimestepEndTime(const double& globalTimestepEndTime) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._globalTimestepEndTime = globalTimestepEndTime;
                  }
                  
                  
                  
                  inline bool getAllPatchesEvolvedToGlobalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (17);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setAllPatchesEvolvedToGlobalTimestep(const bool& allPatchesEvolvedToGlobalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (17);
   _persistentRecords._packedRecords0 = static_cast<int>( allPatchesEvolvedToGlobalTimestep ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._domainOffset;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._domainOffset = (domainOffset);
                  }
                  
                  
                  
                  inline double getDomainOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._domainOffset[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setDomainOffset(int elementIndex, const double& domainOffset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._domainOffset[elementIndex]= domainOffset;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getDomainSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._domainSize;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setDomainSize(const tarch::la::Vector<DIMENSIONS,double>& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._domainSize = (domainSize);
                  }
                  
                  
                  
                  inline double getDomainSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._domainSize[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setDomainSize(int elementIndex, const double& domainSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._domainSize[elementIndex]= domainSize;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<32,int> getPlotName() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._plotName;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setPlotName(const tarch::la::Vector<32,int>& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._plotName = (plotName);
                  }
                  
                  
                  
                  inline int getPlotName(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<32);
                     return _persistentRecords._plotName[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setPlotName(int elementIndex, const int& plotName) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<32);
                     _persistentRecords._plotName[elementIndex]= plotName;
                     
                  }
                  
                  
                  
                  inline int getPlotNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._plotNumber;
                  }
                  
                  
                  
                  inline void setPlotNumber(const int& plotNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._plotNumber = plotNumber;
                  }
                  
                  
                  
                  inline double getStartMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._startMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setStartMaximumGlobalTimeInterval(const double& startMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._startMaximumGlobalTimeInterval = startMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getEndMaximumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._endMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setEndMaximumGlobalTimeInterval(const double& endMaximumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._endMaximumGlobalTimeInterval = endMaximumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getStartMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._startMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setStartMinimumGlobalTimeInterval(const double& startMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._startMinimumGlobalTimeInterval = startMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getEndMinimumGlobalTimeInterval() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._endMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline void setEndMinimumGlobalTimeInterval(const double& endMinimumGlobalTimeInterval) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._endMinimumGlobalTimeInterval = endMinimumGlobalTimeInterval;
                  }
                  
                  
                  
                  inline double getMinimalTimestep() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._minimalTimestep;
                  }
                  
                  
                  
                  inline void setMinimalTimestep(const double& minimalTimestep) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._minimalTimestep = minimalTimestep;
                  }
                  
                  
                  
                  inline double getTotalNumberOfCellUpdates() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._totalNumberOfCellUpdates;
                  }
                  
                  
                  
                  inline void setTotalNumberOfCellUpdates(const double& totalNumberOfCellUpdates) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._totalNumberOfCellUpdates = totalNumberOfCellUpdates;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getMinMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._minMeshWidth;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setMinMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._minMeshWidth = (minMeshWidth);
                  }
                  
                  
                  
                  inline double getMinMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._minMeshWidth[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setMinMeshWidth(int elementIndex, const double& minMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._minMeshWidth[elementIndex]= minMeshWidth;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getMaxMeshWidth() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._maxMeshWidth;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setMaxMeshWidth(const tarch::la::Vector<DIMENSIONS,double>& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._maxMeshWidth = (maxMeshWidth);
                  }
                  
                  
                  
                  inline double getMaxMeshWidth(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._maxMeshWidth[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setMaxMeshWidth(int elementIndex, const double& maxMeshWidth) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._maxMeshWidth[elementIndex]= maxMeshWidth;
                     
                  }
                  
                  
                  
                  inline double getNumberOfInnerVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfInnerVertices;
                  }
                  
                  
                  
                  inline void setNumberOfInnerVertices(const double& numberOfInnerVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfInnerVertices = numberOfInnerVertices;
                  }
                  
                  
                  
                  inline double getNumberOfBoundaryVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfBoundaryVertices;
                  }
                  
                  
                  
                  inline void setNumberOfBoundaryVertices(const double& numberOfBoundaryVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfBoundaryVertices = numberOfBoundaryVertices;
                  }
                  
                  
                  
                  inline double getNumberOfOuterVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfOuterVertices;
                  }
                  
                  
                  
                  inline void setNumberOfOuterVertices(const double& numberOfOuterVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfOuterVertices = numberOfOuterVertices;
                  }
                  
                  
                  
                  inline double getNumberOfInnerCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfInnerCells;
                  }
                  
                  
                  
                  inline void setNumberOfInnerCells(const double& numberOfInnerCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfInnerCells = numberOfInnerCells;
                  }
                  
                  
                  
                  inline double getNumberOfOuterCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfOuterCells;
                  }
                  
                  
                  
                  inline void setNumberOfOuterCells(const double& numberOfOuterCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfOuterCells = numberOfOuterCells;
                  }
                  
                  
                  
                  inline double getNumberOfInnerLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfInnerLeafVertices;
                  }
                  
                  
                  
                  inline void setNumberOfInnerLeafVertices(const double& numberOfInnerLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfInnerLeafVertices = numberOfInnerLeafVertices;
                  }
                  
                  
                  
                  inline double getNumberOfBoundaryLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfBoundaryLeafVertices;
                  }
                  
                  
                  
                  inline void setNumberOfBoundaryLeafVertices(const double& numberOfBoundaryLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfBoundaryLeafVertices = numberOfBoundaryLeafVertices;
                  }
                  
                  
                  
                  inline double getNumberOfOuterLeafVertices() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfOuterLeafVertices;
                  }
                  
                  
                  
                  inline void setNumberOfOuterLeafVertices(const double& numberOfOuterLeafVertices) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfOuterLeafVertices = numberOfOuterLeafVertices;
                  }
                  
                  
                  
                  inline double getNumberOfInnerLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfInnerLeafCells;
                  }
                  
                  
                  
                  inline void setNumberOfInnerLeafCells(const double& numberOfInnerLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfInnerLeafCells = numberOfInnerLeafCells;
                  }
                  
                  
                  
                  inline double getNumberOfOuterLeafCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfOuterLeafCells;
                  }
                  
                  
                  
                  inline void setNumberOfOuterLeafCells(const double& numberOfOuterLeafCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfOuterLeafCells = numberOfOuterLeafCells;
                  }
                  
                  
                  
                  inline int getMaxLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._maxLevel;
                  }
                  
                  
                  
                  inline void setMaxLevel(const int& maxLevel) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._maxLevel = maxLevel;
                  }
                  
                  
                  
                  inline bool getHasRefined() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (18);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setHasRefined(const bool& hasRefined) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (18);
   _persistentRecords._packedRecords0 = static_cast<int>( hasRefined ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getHasTriggeredRefinementForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (19);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setHasTriggeredRefinementForNextIteration(const bool& hasTriggeredRefinementForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (19);
   _persistentRecords._packedRecords0 = static_cast<int>( hasTriggeredRefinementForNextIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getHasErased() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (20);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setHasErased(const bool& hasErased) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (20);
   _persistentRecords._packedRecords0 = static_cast<int>( hasErased ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getHasTriggeredEraseForNextIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (21);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setHasTriggeredEraseForNextIteration(const bool& hasTriggeredEraseForNextIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (21);
   _persistentRecords._packedRecords0 = static_cast<int>( hasTriggeredEraseForNextIteration ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getHasChangedVertexOrCellState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (22);
   int tmp = static_cast<int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setHasChangedVertexOrCellState(const bool& hasChangedVertexOrCellState) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     int mask = 1 << (22);
   _persistentRecords._packedRecords0 = static_cast<int>( hasChangedVertexOrCellState ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getIsTraversalInverted() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isTraversalInverted;
                  }
                  
                  
                  
                  inline void setIsTraversalInverted(const bool& isTraversalInverted) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isTraversalInverted = isTraversalInverted;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  State convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, bool communicateBlocking);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifdef PackedRecords
               #pragma pack (pop)
               #endif
               
               
               
            
         #endif
         
         #endif
         
